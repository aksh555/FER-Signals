{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Inception-v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xpse6e-ljcR",
        "colab_type": "code",
        "outputId": "acb27cae-f26f-4d52-b0ad-1df573790128",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVz-7mXdWREx",
        "colab_type": "code",
        "outputId": "0c5bb2aa-7a45-4cb5-f3f7-45fb8794fd01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 603
        }
      },
      "source": [
        "# DEPENDENCIES ########################################################################################################################################\n",
        "\n",
        "!pip install keras\n",
        "!pip install pandas\n",
        "!pip install scikit_image\n",
        "!pip install h5py"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.3)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: scikit_image in /usr/local/lib/python3.6/dist-packages (0.15.0)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit_image) (3.1.1)\n",
            "Requirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit_image) (1.1.1)\n",
            "Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from scikit_image) (2.4.1)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit_image) (2.4)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit_image) (1.3.1)\n",
            "Requirement already satisfied: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit_image) (4.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit_image) (2.6.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit_image) (2.4.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit_image) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit_image) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit_image) (1.17.3)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit_image) (4.4.1)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow>=4.3.0->scikit_image) (0.46)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit_image) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit_image) (41.4.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from h5py) (1.17.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnhYslJdWRE2",
        "colab_type": "code",
        "outputId": "d5e33c6a-f167-4619-ed78-f45025735dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "# IMPORTS #############################################################################################################################################\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.python.lib.io import file_io\n",
        "from skimage.transform import resize\n",
        "from keras.utils import to_categorical\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras.models import Model\n",
        "from keras.layers import GlobalAveragePooling2D, Dense \n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.callbacks import TensorBoard, ReduceLROnPlateau, EarlyStopping, Callback\n",
        "import h5py # For saving the model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_n7PF9vDWRE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# PARAMETERS ##########################################################################################################################################\n",
        "\n",
        "# Size of the images\n",
        "img_height, img_width = 139, 139\n",
        "\n",
        "# Parameters\n",
        "num_classes         = 7     # ['Anger', 'Disgust', 'Fear', 'Happiness', 'Sadness', 'Surprise', 'Neutral']\n",
        "epochs_top_layers   = 5\n",
        "epochs_all_layers   = 100\n",
        "batch_size          = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXpeB6tSWRE7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATASETS ############################################################################################################################################\n",
        "\n",
        "# Folder where logs and models are stored\n",
        "folder = 'drive/My Drive/Colab Notebooks'\n",
        "\n",
        "# Data paths\n",
        "train_dataset\t= 'drive/My Drive/Colab Notebooks/fer2013_train.csv'\n",
        "eval_dataset \t= 'drive/My Drive/Colab Notebooks/fer2013_eval.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xde_i4euiWc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " class FileWrap:\n",
        "    def __init__(self, path):\n",
        "        self.file = open(path)\n",
        "    def __iter__(self):\n",
        "        self.file.readline().rstrip()\n",
        "    def read(self, *args, **kwargs):\n",
        "        return self.file.read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "7CrfRGTxWRE9",
        "colab_type": "code",
        "outputId": "cd944547-25c7-4f25-c84d-9dab5d65a0c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# MODEL ###############################################################################################################################################\n",
        "\n",
        "# Create the base pre-trained model\n",
        "    # include_top:  Whether to include the fully-connected layer at the top of the network\n",
        "    # weights:      Pre-training on ImageNet\n",
        "    # input_shape:  Optional shape tuple, only to be specified if include_top is False (otherwise the input shape has to be (299, 299, 3) \n",
        "    #               (with 'channels_last' data format) or (3, 299, 299) (with 'channels_first' data format). It should have exactly 3 inputs channels, \n",
        "    #               and width and height should be no smaller than 139. E.g. (150, 150, 3) would be one valid value\n",
        "# Returns a keras Model instance\n",
        "base_model = InceptionV3(\n",
        "    include_top = False,\n",
        "    weights     = 'imagenet',\n",
        "    input_shape = (img_height, img_width, 3))\n",
        "\n",
        "# Places x as the output of the pre-trained model\n",
        "x = base_model.output\n",
        "\n",
        "# Add a global spatial average pooling layer\n",
        "    # data_format:  The ordering of the dimensions in the inputs. Default: channels_last (batch, height, width, channels)\n",
        "    # input shape:  4D tensor with shape: (batch_size, rows, cols, channels)\n",
        "    # output shape: 2D tensor with shape: (batch_size, channels) => (None, 2048)\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "# Add a fully-connected layer and a logistic layer\n",
        "# Dense implements the operation: output = activation(dot(input, kernel) + bias(only applicable if use_bias is True))\n",
        "    # units:        Positive integer, dimensionality of the output space\n",
        "    # activation:   Activation function to use\n",
        "    # input shape:  nD tensor with shape: (batch_size, ..., input_dim)\n",
        "    # output shape: nD tensor with shape: (batch_size, ..., units)\n",
        "x = Dense(1024, activation = 'relu')(x)\n",
        "predictions = Dense(num_classes, activation = 'softmax')(x)\n",
        "\n",
        "# The model we will train\n",
        "model = Model(inputs = base_model.input, outputs = predictions)\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 139, 139, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 69, 69, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 69, 69, 32)   96          conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_1 (Activation)       (None, 69, 69, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 67, 67, 32)   9216        activation_1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 67, 67, 32)   96          conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_2 (Activation)       (None, 67, 67, 32)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 67, 67, 64)   18432       activation_2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 67, 67, 64)   192         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_3 (Activation)       (None, 67, 67, 64)   0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 33, 33, 64)   0           activation_3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 33, 33, 80)   5120        max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 33, 33, 80)   240         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_4 (Activation)       (None, 33, 33, 80)   0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 31, 31, 192)  138240      activation_4[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 31, 31, 192)  576         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_5 (Activation)       (None, 31, 31, 192)  0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 15, 15, 192)  0           activation_5[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 15, 15, 64)   12288       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 15, 15, 64)   192         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "activation_9 (Activation)       (None, 15, 15, 64)   0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 15, 15, 48)   9216        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 15, 15, 96)   55296       activation_9[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 15, 15, 48)   144         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 15, 15, 96)   288         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_7 (Activation)       (None, 15, 15, 48)   0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_10 (Activation)      (None, 15, 15, 96)   0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 15, 15, 192)  0           max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 15, 15, 64)   12288       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 15, 15, 64)   76800       activation_7[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 15, 15, 96)   82944       activation_10[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 15, 15, 32)   6144        average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 15, 15, 64)   192         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 15, 15, 64)   192         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 15, 15, 96)   288         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 15, 15, 32)   96          conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_6 (Activation)       (None, 15, 15, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_8 (Activation)       (None, 15, 15, 64)   0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "activation_11 (Activation)      (None, 15, 15, 96)   0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_12 (Activation)      (None, 15, 15, 32)   0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed0 (Concatenate)            (None, 15, 15, 256)  0           activation_6[0][0]               \n",
            "                                                                 activation_8[0][0]               \n",
            "                                                                 activation_11[0][0]              \n",
            "                                                                 activation_12[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 15, 15, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 15, 15, 64)   192         conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_16 (Activation)      (None, 15, 15, 64)   0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 15, 15, 48)   12288       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 15, 15, 96)   55296       activation_16[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 15, 15, 48)   144         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 15, 15, 96)   288         conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_14 (Activation)      (None, 15, 15, 48)   0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_17 (Activation)      (None, 15, 15, 96)   0           batch_normalization_17[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_2 (AveragePoo (None, 15, 15, 256)  0           mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 15, 15, 64)   16384       mixed0[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 15, 15, 64)   76800       activation_14[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 15, 15, 96)   82944       activation_17[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 15, 15, 64)   16384       average_pooling2d_2[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 15, 15, 64)   192         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 15, 15, 64)   192         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 15, 15, 96)   288         conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 15, 15, 64)   192         conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_13 (Activation)      (None, 15, 15, 64)   0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_15 (Activation)      (None, 15, 15, 64)   0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_18 (Activation)      (None, 15, 15, 96)   0           batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_19 (Activation)      (None, 15, 15, 64)   0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed1 (Concatenate)            (None, 15, 15, 288)  0           activation_13[0][0]              \n",
            "                                                                 activation_15[0][0]              \n",
            "                                                                 activation_18[0][0]              \n",
            "                                                                 activation_19[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 15, 15, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 15, 15, 64)   192         conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_23 (Activation)      (None, 15, 15, 64)   0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 15, 15, 48)   13824       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 15, 15, 96)   55296       activation_23[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 15, 15, 48)   144         conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 15, 15, 96)   288         conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_21 (Activation)      (None, 15, 15, 48)   0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_24 (Activation)      (None, 15, 15, 96)   0           batch_normalization_24[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_3 (AveragePoo (None, 15, 15, 288)  0           mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 15, 15, 64)   18432       mixed1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 15, 15, 64)   76800       activation_21[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 15, 15, 96)   82944       activation_24[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 15, 15, 64)   18432       average_pooling2d_3[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 15, 15, 64)   192         conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 15, 15, 64)   192         conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 15, 15, 96)   288         conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 15, 15, 64)   192         conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_20 (Activation)      (None, 15, 15, 64)   0           batch_normalization_20[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_22 (Activation)      (None, 15, 15, 64)   0           batch_normalization_22[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_25 (Activation)      (None, 15, 15, 96)   0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_26 (Activation)      (None, 15, 15, 64)   0           batch_normalization_26[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed2 (Concatenate)            (None, 15, 15, 288)  0           activation_20[0][0]              \n",
            "                                                                 activation_22[0][0]              \n",
            "                                                                 activation_25[0][0]              \n",
            "                                                                 activation_26[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 15, 15, 64)   18432       mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 15, 15, 64)   192         conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_28 (Activation)      (None, 15, 15, 64)   0           batch_normalization_28[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 15, 15, 96)   55296       activation_28[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 15, 15, 96)   288         conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_29 (Activation)      (None, 15, 15, 96)   0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 7, 7, 384)    995328      mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 7, 7, 96)     82944       activation_29[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 7, 7, 384)    1152        conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 7, 7, 96)     288         conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_27 (Activation)      (None, 7, 7, 384)    0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_30 (Activation)      (None, 7, 7, 96)     0           batch_normalization_30[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 7, 7, 288)    0           mixed2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed3 (Concatenate)            (None, 7, 7, 768)    0           activation_27[0][0]              \n",
            "                                                                 activation_30[0][0]              \n",
            "                                                                 max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 7, 7, 128)    384         conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_35 (Activation)      (None, 7, 7, 128)    0           batch_normalization_35[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_36 (Conv2D)              (None, 7, 7, 128)    114688      activation_35[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_36 (BatchNo (None, 7, 7, 128)    384         conv2d_36[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_36 (Activation)      (None, 7, 7, 128)    0           batch_normalization_36[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 7, 7, 128)    98304       mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_37 (Conv2D)              (None, 7, 7, 128)    114688      activation_36[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 7, 7, 128)    384         conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_37 (BatchNo (None, 7, 7, 128)    384         conv2d_37[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_32 (Activation)      (None, 7, 7, 128)    0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_37 (Activation)      (None, 7, 7, 128)    0           batch_normalization_37[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 7, 7, 128)    114688      activation_32[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_38 (Conv2D)              (None, 7, 7, 128)    114688      activation_37[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 7, 7, 128)    384         conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_38 (BatchNo (None, 7, 7, 128)    384         conv2d_38[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_33 (Activation)      (None, 7, 7, 128)    0           batch_normalization_33[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_38 (Activation)      (None, 7, 7, 128)    0           batch_normalization_38[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_4 (AveragePoo (None, 7, 7, 768)    0           mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 7, 7, 192)    147456      mixed3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 7, 7, 192)    172032      activation_33[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_39 (Conv2D)              (None, 7, 7, 192)    172032      activation_38[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_40 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_4[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 7, 7, 192)    576         conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 7, 7, 192)    576         conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_39 (BatchNo (None, 7, 7, 192)    576         conv2d_39[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_40 (BatchNo (None, 7, 7, 192)    576         conv2d_40[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_31 (Activation)      (None, 7, 7, 192)    0           batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_34 (Activation)      (None, 7, 7, 192)    0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_39 (Activation)      (None, 7, 7, 192)    0           batch_normalization_39[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_40 (Activation)      (None, 7, 7, 192)    0           batch_normalization_40[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed4 (Concatenate)            (None, 7, 7, 768)    0           activation_31[0][0]              \n",
            "                                                                 activation_34[0][0]              \n",
            "                                                                 activation_39[0][0]              \n",
            "                                                                 activation_40[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_45 (Conv2D)              (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_45 (BatchNo (None, 7, 7, 160)    480         conv2d_45[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_45 (Activation)      (None, 7, 7, 160)    0           batch_normalization_45[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_46 (Conv2D)              (None, 7, 7, 160)    179200      activation_45[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_46 (BatchNo (None, 7, 7, 160)    480         conv2d_46[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_46 (Activation)      (None, 7, 7, 160)    0           batch_normalization_46[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_42 (Conv2D)              (None, 7, 7, 160)    122880      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_47 (Conv2D)              (None, 7, 7, 160)    179200      activation_46[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_42 (BatchNo (None, 7, 7, 160)    480         conv2d_42[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_47 (BatchNo (None, 7, 7, 160)    480         conv2d_47[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_42 (Activation)      (None, 7, 7, 160)    0           batch_normalization_42[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_47 (Activation)      (None, 7, 7, 160)    0           batch_normalization_47[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_43 (Conv2D)              (None, 7, 7, 160)    179200      activation_42[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_48 (Conv2D)              (None, 7, 7, 160)    179200      activation_47[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_43 (BatchNo (None, 7, 7, 160)    480         conv2d_43[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_48 (BatchNo (None, 7, 7, 160)    480         conv2d_48[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_43 (Activation)      (None, 7, 7, 160)    0           batch_normalization_43[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_48 (Activation)      (None, 7, 7, 160)    0           batch_normalization_48[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_5 (AveragePoo (None, 7, 7, 768)    0           mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_41 (Conv2D)              (None, 7, 7, 192)    147456      mixed4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_44 (Conv2D)              (None, 7, 7, 192)    215040      activation_43[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_49 (Conv2D)              (None, 7, 7, 192)    215040      activation_48[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_50 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_5[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_41 (BatchNo (None, 7, 7, 192)    576         conv2d_41[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_44 (BatchNo (None, 7, 7, 192)    576         conv2d_44[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_49 (BatchNo (None, 7, 7, 192)    576         conv2d_49[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_50 (BatchNo (None, 7, 7, 192)    576         conv2d_50[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_41 (Activation)      (None, 7, 7, 192)    0           batch_normalization_41[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_44 (Activation)      (None, 7, 7, 192)    0           batch_normalization_44[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_49 (Activation)      (None, 7, 7, 192)    0           batch_normalization_49[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_50 (Activation)      (None, 7, 7, 192)    0           batch_normalization_50[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed5 (Concatenate)            (None, 7, 7, 768)    0           activation_41[0][0]              \n",
            "                                                                 activation_44[0][0]              \n",
            "                                                                 activation_49[0][0]              \n",
            "                                                                 activation_50[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_55 (Conv2D)              (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_55 (BatchNo (None, 7, 7, 160)    480         conv2d_55[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_55 (Activation)      (None, 7, 7, 160)    0           batch_normalization_55[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_56 (Conv2D)              (None, 7, 7, 160)    179200      activation_55[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_56 (BatchNo (None, 7, 7, 160)    480         conv2d_56[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_56 (Activation)      (None, 7, 7, 160)    0           batch_normalization_56[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_52 (Conv2D)              (None, 7, 7, 160)    122880      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_57 (Conv2D)              (None, 7, 7, 160)    179200      activation_56[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_52 (BatchNo (None, 7, 7, 160)    480         conv2d_52[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_57 (BatchNo (None, 7, 7, 160)    480         conv2d_57[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_52 (Activation)      (None, 7, 7, 160)    0           batch_normalization_52[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_57 (Activation)      (None, 7, 7, 160)    0           batch_normalization_57[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_53 (Conv2D)              (None, 7, 7, 160)    179200      activation_52[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_58 (Conv2D)              (None, 7, 7, 160)    179200      activation_57[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_53 (BatchNo (None, 7, 7, 160)    480         conv2d_53[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_58 (BatchNo (None, 7, 7, 160)    480         conv2d_58[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_53 (Activation)      (None, 7, 7, 160)    0           batch_normalization_53[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_58 (Activation)      (None, 7, 7, 160)    0           batch_normalization_58[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_6 (AveragePoo (None, 7, 7, 768)    0           mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_51 (Conv2D)              (None, 7, 7, 192)    147456      mixed5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_54 (Conv2D)              (None, 7, 7, 192)    215040      activation_53[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_59 (Conv2D)              (None, 7, 7, 192)    215040      activation_58[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_60 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_6[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_51 (BatchNo (None, 7, 7, 192)    576         conv2d_51[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_54 (BatchNo (None, 7, 7, 192)    576         conv2d_54[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_59 (BatchNo (None, 7, 7, 192)    576         conv2d_59[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_60 (BatchNo (None, 7, 7, 192)    576         conv2d_60[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_51 (Activation)      (None, 7, 7, 192)    0           batch_normalization_51[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_54 (Activation)      (None, 7, 7, 192)    0           batch_normalization_54[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_59 (Activation)      (None, 7, 7, 192)    0           batch_normalization_59[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_60 (Activation)      (None, 7, 7, 192)    0           batch_normalization_60[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed6 (Concatenate)            (None, 7, 7, 768)    0           activation_51[0][0]              \n",
            "                                                                 activation_54[0][0]              \n",
            "                                                                 activation_59[0][0]              \n",
            "                                                                 activation_60[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_65 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_65 (BatchNo (None, 7, 7, 192)    576         conv2d_65[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_65 (Activation)      (None, 7, 7, 192)    0           batch_normalization_65[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_66 (Conv2D)              (None, 7, 7, 192)    258048      activation_65[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_66 (BatchNo (None, 7, 7, 192)    576         conv2d_66[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_66 (Activation)      (None, 7, 7, 192)    0           batch_normalization_66[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_62 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_67 (Conv2D)              (None, 7, 7, 192)    258048      activation_66[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_62 (BatchNo (None, 7, 7, 192)    576         conv2d_62[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_67 (BatchNo (None, 7, 7, 192)    576         conv2d_67[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_62 (Activation)      (None, 7, 7, 192)    0           batch_normalization_62[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_67 (Activation)      (None, 7, 7, 192)    0           batch_normalization_67[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_63 (Conv2D)              (None, 7, 7, 192)    258048      activation_62[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_68 (Conv2D)              (None, 7, 7, 192)    258048      activation_67[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_63 (BatchNo (None, 7, 7, 192)    576         conv2d_63[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_68 (BatchNo (None, 7, 7, 192)    576         conv2d_68[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_63 (Activation)      (None, 7, 7, 192)    0           batch_normalization_63[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_68 (Activation)      (None, 7, 7, 192)    0           batch_normalization_68[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_7 (AveragePoo (None, 7, 7, 768)    0           mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_61 (Conv2D)              (None, 7, 7, 192)    147456      mixed6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_64 (Conv2D)              (None, 7, 7, 192)    258048      activation_63[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_69 (Conv2D)              (None, 7, 7, 192)    258048      activation_68[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_70 (Conv2D)              (None, 7, 7, 192)    147456      average_pooling2d_7[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_61 (BatchNo (None, 7, 7, 192)    576         conv2d_61[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_64 (BatchNo (None, 7, 7, 192)    576         conv2d_64[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_69 (BatchNo (None, 7, 7, 192)    576         conv2d_69[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_70 (BatchNo (None, 7, 7, 192)    576         conv2d_70[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_61 (Activation)      (None, 7, 7, 192)    0           batch_normalization_61[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_64 (Activation)      (None, 7, 7, 192)    0           batch_normalization_64[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_69 (Activation)      (None, 7, 7, 192)    0           batch_normalization_69[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_70 (Activation)      (None, 7, 7, 192)    0           batch_normalization_70[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed7 (Concatenate)            (None, 7, 7, 768)    0           activation_61[0][0]              \n",
            "                                                                 activation_64[0][0]              \n",
            "                                                                 activation_69[0][0]              \n",
            "                                                                 activation_70[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_73 (Conv2D)              (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_73 (BatchNo (None, 7, 7, 192)    576         conv2d_73[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_73 (Activation)      (None, 7, 7, 192)    0           batch_normalization_73[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_74 (Conv2D)              (None, 7, 7, 192)    258048      activation_73[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_74 (BatchNo (None, 7, 7, 192)    576         conv2d_74[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_74 (Activation)      (None, 7, 7, 192)    0           batch_normalization_74[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_71 (Conv2D)              (None, 7, 7, 192)    147456      mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_75 (Conv2D)              (None, 7, 7, 192)    258048      activation_74[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_71 (BatchNo (None, 7, 7, 192)    576         conv2d_71[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_75 (BatchNo (None, 7, 7, 192)    576         conv2d_75[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_71 (Activation)      (None, 7, 7, 192)    0           batch_normalization_71[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_75 (Activation)      (None, 7, 7, 192)    0           batch_normalization_75[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_72 (Conv2D)              (None, 3, 3, 320)    552960      activation_71[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_76 (Conv2D)              (None, 3, 3, 192)    331776      activation_75[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_72 (BatchNo (None, 3, 3, 320)    960         conv2d_72[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_76 (BatchNo (None, 3, 3, 192)    576         conv2d_76[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_72 (Activation)      (None, 3, 3, 320)    0           batch_normalization_72[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_76 (Activation)      (None, 3, 3, 192)    0           batch_normalization_76[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 3, 3, 768)    0           mixed7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "mixed8 (Concatenate)            (None, 3, 3, 1280)   0           activation_72[0][0]              \n",
            "                                                                 activation_76[0][0]              \n",
            "                                                                 max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_81 (Conv2D)              (None, 3, 3, 448)    573440      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_81 (BatchNo (None, 3, 3, 448)    1344        conv2d_81[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_81 (Activation)      (None, 3, 3, 448)    0           batch_normalization_81[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_78 (Conv2D)              (None, 3, 3, 384)    491520      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_82 (Conv2D)              (None, 3, 3, 384)    1548288     activation_81[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_78 (BatchNo (None, 3, 3, 384)    1152        conv2d_78[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_82 (BatchNo (None, 3, 3, 384)    1152        conv2d_82[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_78 (Activation)      (None, 3, 3, 384)    0           batch_normalization_78[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_82 (Activation)      (None, 3, 3, 384)    0           batch_normalization_82[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_79 (Conv2D)              (None, 3, 3, 384)    442368      activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_80 (Conv2D)              (None, 3, 3, 384)    442368      activation_78[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_83 (Conv2D)              (None, 3, 3, 384)    442368      activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_84 (Conv2D)              (None, 3, 3, 384)    442368      activation_82[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_8 (AveragePoo (None, 3, 3, 1280)   0           mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_77 (Conv2D)              (None, 3, 3, 320)    409600      mixed8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_79 (BatchNo (None, 3, 3, 384)    1152        conv2d_79[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_80 (BatchNo (None, 3, 3, 384)    1152        conv2d_80[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_83 (BatchNo (None, 3, 3, 384)    1152        conv2d_83[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_84 (BatchNo (None, 3, 3, 384)    1152        conv2d_84[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_85 (Conv2D)              (None, 3, 3, 192)    245760      average_pooling2d_8[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_77 (BatchNo (None, 3, 3, 320)    960         conv2d_77[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_79 (Activation)      (None, 3, 3, 384)    0           batch_normalization_79[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_80 (Activation)      (None, 3, 3, 384)    0           batch_normalization_80[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_83 (Activation)      (None, 3, 3, 384)    0           batch_normalization_83[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_84 (Activation)      (None, 3, 3, 384)    0           batch_normalization_84[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_85 (BatchNo (None, 3, 3, 192)    576         conv2d_85[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_77 (Activation)      (None, 3, 3, 320)    0           batch_normalization_77[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_0 (Concatenate)          (None, 3, 3, 768)    0           activation_79[0][0]              \n",
            "                                                                 activation_80[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 3, 3, 768)    0           activation_83[0][0]              \n",
            "                                                                 activation_84[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_85 (Activation)      (None, 3, 3, 192)    0           batch_normalization_85[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9 (Concatenate)            (None, 3, 3, 2048)   0           activation_77[0][0]              \n",
            "                                                                 mixed9_0[0][0]                   \n",
            "                                                                 concatenate_1[0][0]              \n",
            "                                                                 activation_85[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_90 (Conv2D)              (None, 3, 3, 448)    917504      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_90 (BatchNo (None, 3, 3, 448)    1344        conv2d_90[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_90 (Activation)      (None, 3, 3, 448)    0           batch_normalization_90[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_87 (Conv2D)              (None, 3, 3, 384)    786432      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_91 (Conv2D)              (None, 3, 3, 384)    1548288     activation_90[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_87 (BatchNo (None, 3, 3, 384)    1152        conv2d_87[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_91 (BatchNo (None, 3, 3, 384)    1152        conv2d_91[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_87 (Activation)      (None, 3, 3, 384)    0           batch_normalization_87[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_91 (Activation)      (None, 3, 3, 384)    0           batch_normalization_91[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_88 (Conv2D)              (None, 3, 3, 384)    442368      activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_89 (Conv2D)              (None, 3, 3, 384)    442368      activation_87[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_92 (Conv2D)              (None, 3, 3, 384)    442368      activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_93 (Conv2D)              (None, 3, 3, 384)    442368      activation_91[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_9 (AveragePoo (None, 3, 3, 2048)   0           mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_86 (Conv2D)              (None, 3, 3, 320)    655360      mixed9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_88 (BatchNo (None, 3, 3, 384)    1152        conv2d_88[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_89 (BatchNo (None, 3, 3, 384)    1152        conv2d_89[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_92 (BatchNo (None, 3, 3, 384)    1152        conv2d_92[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_93 (BatchNo (None, 3, 3, 384)    1152        conv2d_93[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_94 (Conv2D)              (None, 3, 3, 192)    393216      average_pooling2d_9[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_86 (BatchNo (None, 3, 3, 320)    960         conv2d_86[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_88 (Activation)      (None, 3, 3, 384)    0           batch_normalization_88[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_89 (Activation)      (None, 3, 3, 384)    0           batch_normalization_89[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_92 (Activation)      (None, 3, 3, 384)    0           batch_normalization_92[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "activation_93 (Activation)      (None, 3, 3, 384)    0           batch_normalization_93[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_94 (BatchNo (None, 3, 3, 192)    576         conv2d_94[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "activation_86 (Activation)      (None, 3, 3, 320)    0           batch_normalization_86[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed9_1 (Concatenate)          (None, 3, 3, 768)    0           activation_88[0][0]              \n",
            "                                                                 activation_89[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_2 (Concatenate)     (None, 3, 3, 768)    0           activation_92[0][0]              \n",
            "                                                                 activation_93[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "activation_94 (Activation)      (None, 3, 3, 192)    0           batch_normalization_94[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "mixed10 (Concatenate)           (None, 3, 3, 2048)   0           activation_86[0][0]              \n",
            "                                                                 mixed9_1[0][0]                   \n",
            "                                                                 concatenate_2[0][0]              \n",
            "                                                                 activation_94[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 2048)         0           mixed10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 1024)         2098176     global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 7)            7175        dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 23,908,135\n",
            "Trainable params: 23,873,703\n",
            "Non-trainable params: 34,432\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5uEygy_WRE_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DATA PREPARATION ####################################################################################################################################\n",
        "\n",
        "# Preprocesses a numpy array encoding a batch of images\n",
        "    # x: Input array to preprocess\n",
        "def preprocess_input(x):\n",
        "    x /= 127.5\n",
        "    x -= 1.\n",
        "    return x\n",
        "\n",
        "# Function that reads the data from the csv file, increases the size of the images and returns the images and their labels\n",
        "    # dataset: Data path\n",
        "def get_data(dataset):\n",
        "\n",
        "    #file_stream = file_io.FileIO(dataset, mode='r')\n",
        "    data = pd.read_csv(dataset)\n",
        "    pixels = data['pixels'].tolist()\n",
        "    images = np.empty((len(data), img_height, img_width, 3))\n",
        "    i = 0\n",
        "\n",
        "    for pixel_sequence in pixels:\n",
        "        single_image = [float(pixel) for pixel in pixel_sequence.split(' ')]  # Extraction of each single\n",
        "        single_image = np.asarray(single_image).reshape(48, 48) # Dimension: 48x48\n",
        "        single_image = resize(single_image, (img_height, img_width), order = 3, mode = 'constant') # Dimension: 139x139x3 (Bicubic)\n",
        "        ret = np.empty((img_height, img_width, 3))  \n",
        "        ret[:, :, 0] = single_image\n",
        "        ret[:, :, 1] = single_image\n",
        "        ret[:, :, 2] = single_image\n",
        "        images[i, :, :, :] = ret\n",
        "        i += 1\n",
        "    \n",
        "    images = preprocess_input(images)\n",
        "    labels = to_categorical(data['emotion'])\n",
        "\n",
        "    return images, labels    \n",
        "\n",
        "# Data preparation\n",
        "train_data_x, train_data_y  = get_data(train_dataset)\n",
        "val_data  = get_data(eval_dataset)\n",
        "\n",
        "# Generate batches of tensor image data with real-time data augmentation. The data will be looped over (in batches) indefinitely\n",
        "# rescale:          Rescaling factor (defaults to None). Multiply the data by the value provided (before applying any other transformation)\n",
        "# rotation_range:   Int. Degree range for random rotations\n",
        "# shear_range:      Float. Shear Intensity (Shear angle in counter-clockwise direction as radians)\n",
        "# zoom_range:       Float or [lower, upper]. Range for random zoom. If a float, [lower, upper] = [1-zoom_range, 1+zoom_range]\n",
        "# fill_mode :       Points outside the boundaries of the input are filled according to the given mode: {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}\n",
        "# horizontal_flip:  Boolean. Randomly flip inputs horizontally\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rotation_range  = 10,\n",
        "    shear_range     = 10, # 10 degrees\n",
        "    zoom_range      = 0.1,\n",
        "    fill_mode       = 'reflect',\n",
        "    horizontal_flip = True)\n",
        "\n",
        "# Takes numpy data & label arrays, and generates batches of augmented/normalized data. Yields batcfillhes indefinitely, in an infinite loop\n",
        "    # x:            Data. Should have rank 4. In case of grayscale data, the channels axis should have value 1, and in case of RGB data, \n",
        "    #               it should have value 3\n",
        "    # y:            Labels\n",
        "    # batch_size:   Int (default: 32)\n",
        "train_generator = train_datagen.flow(\n",
        "    train_data_x,\n",
        "    train_data_y,\n",
        "    batch_size  = batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UYXtA5dsyVu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for l in base_model.layers:\n",
        "    l.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u6UNO4Styzt",
        "colab_type": "code",
        "outputId": "80065040-fc81-48c0-e655-e8de1db7af64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "model.compile(\n",
        "    optimizer   = Adam(lr = 1e-3, beta_1 = 0.9, beta_2 = 0.999, epsilon = 1e-08, decay = 0.0), \n",
        "    loss        = 'categorical_crossentropy', \n",
        "    metrics     = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5iVnJUht4sX",
        "colab_type": "code",
        "outputId": "1329e724-7e11-4c30-9476-036f370fd18a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        }
      },
      "source": [
        "tensorboard_top_layers = TensorBoard(\n",
        "\tlog_dir         = folder + '/logs_top_layers',\n",
        "\thistogram_freq  = 0,\n",
        "\twrite_graph     = True,\n",
        "\twrite_grads     = False,\n",
        "\twrite_images    = True)\n",
        "\n",
        "# Train the model on the new data for a few epochs (Fits the model on data yielded batch-by-batch by a Python generator)\n",
        "    # generator:        A generator or an instance of Sequence (keras.utils.Sequence) object in order to avoid duplicate data when using multiprocessing\n",
        "    #                   The output of the generator must be either {a tuple (inputs, targets)} {a tuple (inputs, targets, sample_weights)}\n",
        "    # steps_per_epoch:  Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch\n",
        "    #                   It should typically be equal to the number of unique samples of your dataset divided by the batch size \n",
        "    # epochs:           Integer, total number of iterations on the data\n",
        "    # validation_data:  This can be either {a generator for the validation data } {a tuple (inputs, targets)} {a tuple (inputs, targets, sample_weights)}\n",
        "    # callbacks:        List of callbacks to be called during training (to visualize the files created during training, run in your terminal:\n",
        "    #                   tensorboard --logdir path_to_current_dir/Graph)\n",
        "model.fit_generator(\n",
        "    generator           = train_generator,\n",
        "    steps_per_epoch     = len(train_data_x) // batch_size,  # samples_per_epoch / batch_size\n",
        "    epochs              = epochs_top_layers,                            \n",
        "    validation_data     = val_data,\n",
        "    callbacks           = [tensorboard_top_layers])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1122: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1125: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "Epoch 1/5\n",
            "224/224 [==============================] - 131s 584ms/step - loss: 1.7271 - acc: 0.3372 - val_loss: 2.1941 - val_acc: 0.2455\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1265: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "Epoch 2/5\n",
            "224/224 [==============================] - 126s 561ms/step - loss: 1.5788 - acc: 0.3864 - val_loss: 2.9342 - val_acc: 0.2137\n",
            "Epoch 3/5\n",
            "224/224 [==============================] - 126s 564ms/step - loss: 1.5545 - acc: 0.3961 - val_loss: 2.5760 - val_acc: 0.2432\n",
            "Epoch 4/5\n",
            "224/224 [==============================] - 127s 566ms/step - loss: 1.5361 - acc: 0.4065 - val_loss: 2.7703 - val_acc: 0.2385\n",
            "Epoch 5/5\n",
            "224/224 [==============================] - 125s 559ms/step - loss: 1.5290 - acc: 0.4051 - val_loss: 2.8652 - val_acc: 0.2541\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f19512c3f28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMrKrtZ6WRFB",
        "colab_type": "code",
        "outputId": "898fc599-d96d-4c59-a429-1ea4217b872d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        }
      },
      "source": [
        "/# UPPER LAYERS TRAINING ###############################################################################################################################\n",
        "\n",
        "# First: train only the top layers (which were randomly initialized) freezing all convolutional ResNet50 layers\n",
        "\n",
        "\n",
        "# Compile (configures the model for training) the model (should be done *AFTER* setting layers to non-trainable)\n",
        "    # optimizer:    String (name of optimizer) or optimizer object\n",
        "        # lr:       Float >= 0. Learning rate\n",
        "        # beta_1:   Float, 0 < beta < 1. Generally close to 1\n",
        "        # beta_2:   Float, 0 < beta < 1. Generally close to 1\n",
        "        # epsilon:  Float >= 0. Fuzz factor\n",
        "        # decay:    Float >= 0. Learning rate decay over each update\n",
        "    # loss:     String (name of objective function) or objective function\n",
        "    # metrics:  List of metrics to be evaluated by the model during training and testing\n",
        "\n",
        "\n",
        "# This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, \n",
        "# as well as activation histograms for the different layers in your model\n",
        "    # log_dir:          The path of the directory where to save the log files to be parsed by TensorBoard\n",
        "    # histogram_freq:   Frequency (in epochs) at which to compute activation and weight histograms for the layers of the model\n",
        "    #                   If set to 0, histograms won't be computed. Validation data (or split) must be specified for histogram visualizations\n",
        "    # write_graph:      Whether to visualize the graph in TensorBoard. The log file can become quite large when write_graph is set to True\n",
        "    # write_grads:      Whether to visualize gradient histograms in TensorBoard. histogram_freq must be greater than 0\n",
        "    # write_images:     Whether to write model weights to visualize as image in TensorBoard\n",
        "# To visualize the files created during training, run in your terminal: tensorboard --logdir path_to_current_dir/Graph\n",
        "tensorboard_top_layers = TensorBoard(\n",
        "\tlog_dir         = folder + '/logs_top_layers',\n",
        "\thistogram_freq  = 0,\n",
        "\twrite_graph     = True,\n",
        "\twrite_grads     = False,\n",
        "\twrite_images    = True)\n",
        "\n",
        "# Train the model on the new data for a few epochs (Fits the model on data yielded batch-by-batch by a Python generator)\n",
        "    # generator:        A generator or an instance of Sequence (keras.utils.Sequence) object in order to avoid duplicate data when using multiprocessing\n",
        "    #                   The output of the generator must be either {a tuple (inputs, targets)} {a tuple (inputs, targets, sample_weights)}\n",
        "    # steps_per_epoch:  Total number of steps (batches of samples) to yield from generator before declaring one epoch finished and starting the next epoch\n",
        "    #                   It should typically be equal to the number of unique samples of your dataset divided by the batch size \n",
        "    # epochs:           Integer, total number of iterations on the data\n",
        "    # validation_data:  This can be either {a generator for the validation data } {a tuple (inputs, targets)} {a tuple (inputs, targets, sample_weights)}\n",
        "    # callbacks:        List of callbacks to be called during training (to visualize the files created during training, run in your terminal:\n",
        "    #                   tensorboard --logdir path_to_current_dir/Graph)\n",
        "model.fit_generator(\n",
        "    generator           = train_generator,\n",
        "    steps_per_epoch     = len(train_data_x) // batch_size,  # samples_per_epoch / batch_size\n",
        "    epochs              = epochs_top_layers,                            \n",
        "    validation_data     = val_data,\n",
        "    callbacks           = [tensorboard_top_layers])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-6ee543adb835>\"\u001b[0;36m, line \u001b[0;32m29\u001b[0m\n\u001b[0;31m    tensorboard_top_layers = TensorBoard(\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T9IfzO1g6Fpl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# FULL NETWORK TRAINING ###############################################################################################################################\n",
        "\n",
        "# At this point, the top layers are well trained and we can start fine-tuning convolutional layers from Inception-v3\n",
        "\n",
        "# Fine-tuning of all the layers\n",
        "for layer in model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# We need to recompile the model for these modifications to take effect (we use SGD with nesterov momentum and a low learning rate)\n",
        "    # optimizer:    String (name of optimizer) or optimizer object\n",
        "        # lr:       float >= 0. Learning rate\n",
        "        # momentum: float >= 0. Parameter updates momentum\n",
        "        # decay:    float >= 0. Learning rate decay over each update\n",
        "        # nesterov: boolean. Whether to apply Nesterov momentum\n",
        "    # loss:     String (name of objective function) or objective function\n",
        "    # metrics:  List of metrics to be evaluated by the model during training and testing\n",
        "model.compile(\n",
        "    optimizer   = SGD(lr = 1e-4, momentum = 0.9, decay = 0.0, nesterov = True),\n",
        "    loss        = 'categorical_crossentropy', \n",
        "    metrics     = ['accuracy'])\n",
        "\n",
        "# This callback writes a log for TensorBoard, which allows you to visualize dynamic graphs of your training and test metrics, \n",
        "tensorboard_all_layers = TensorBoard(\n",
        "    log_dir         = folder + '/logs_all_layers',\n",
        "    histogram_freq  = 0,\n",
        "    write_graph     = True,\n",
        "    write_grads     = False,\n",
        "    write_images    = True)\n",
        "\n",
        "# Reduce learning rate when a metric has stopped improving\n",
        "\t# monitor: \tQuantity to be monitored\n",
        "\t# factor: \tFactor by which the learning rate will be reduced. new_lr = lr * factor\n",
        "\t# patience:\tNumber of epochs with no improvement after which learning rate will be reduced\n",
        "\t# mode: \tOne of {auto, min, max}\n",
        "\t# min_lr:\tLower bound on the learning rate\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "\tmonitor \t= 'val_loss',\n",
        "\tfactor\t\t= 0.4,\n",
        "\tpatience\t= 5,\n",
        "\tmode \t\t= 'auto',\n",
        "\tmin_lr\t\t= 1e-6)\n",
        "\n",
        "# Stop training when a monitored quantity has stopped improving\n",
        "\t# monitor:\t\tQuantity to be monitored\n",
        "\t# patience:\t\tNumber of epochs with no improvement after which training will be stopped\n",
        "\t# mode: \t\tOne of {auto, min, max}\n",
        "early_stop = EarlyStopping(\n",
        "\tmonitor \t= 'val_loss',\n",
        "\tpatience \t= 20,\n",
        "\tmode \t\t= 'auto')\n",
        "\n",
        "class ModelCheckpoint(Callback):\n",
        "\n",
        "\tdef __init__(self, filepath, folder, monitor = 'val_loss', verbose = 0, save_best_only = False, save_weights_only = False, mode = 'auto', period = 1):\n",
        "\t\tsuper(ModelCheckpoint, self).__init__()\n",
        "\t\tself.monitor \t\t\t\t= monitor\n",
        "\t\tself.verbose\t\t \t\t= verbose\n",
        "\t\tself.filepath \t\t\t\t= filepath\n",
        "\t\tself.folder \t\t\t\t= folder\n",
        "\t\tself.save_best_only \t\t= save_best_only\n",
        "\t\tself.save_weights_only\t\t= save_weights_only\n",
        "\t\tself.period \t\t\t\t= period\n",
        "\t\tself.epochs_since_last_save\t= 0\n",
        "\t\t\n",
        "\t\tif mode not in ['auto', 'min', 'max']:\n",
        "\t\t\twarnings.warn('ModelCheckpoint mode %s is unknown, ' 'fallback to auto mode.' % (mode), RuntimeWarning)\n",
        "\t\t\tmode = 'auto'\n",
        "\n",
        "\t\tif mode == 'min':\n",
        "\t\t\tself.monitor_op = np.less\n",
        "\t\t\tself.best = np.Inf\n",
        "\t\telif mode == 'max':\n",
        "\t\t\tself.monitor_op = np.greater\n",
        "\t\t\tself.best = -np.Inf\n",
        "\t\telse:\n",
        "\t\t\tif 'acc' in self.monitor or self.monitor.startswith('fmeasure'):\n",
        "\t\t\t    self.monitor_op = np.greater\n",
        "\t\t\t    self.best = -np.Inf\n",
        "\t\t\telse:\n",
        "\t\t\t    self.monitor_op = np.less\n",
        "\t\t\t    self.best = np.Inf\n",
        "\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DFhjHmblWRFE",
        "colab_type": "code",
        "outputId": "9cff8ffb-0ef1-4b56-f6ef-6fccdc11da00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "def on_epoch_end(self, epoch, logs=None):\n",
        "\tlogs = logs or {}\n",
        "\tself.epochs_since_last_save += 1\n",
        "\tif self.epochs_since_last_save >= self.period:\n",
        "\t\tself.epochs_since_last_save = 0\n",
        "\t\tfilepath = self.filepath.format(epoch = epoch + 1, **logs)\n",
        "\t\tif self.save_best_only:\n",
        "\t\t\tcurrent = logs.get(self.monitor)\n",
        "\t\t\tif current is None:\n",
        "\t\t\t\t\twarnings.warn('Can save best model only with %s available, ' 'skipping.' % (self.monitor), RuntimeWarning)\n",
        "\t\t\telse:\n",
        "\t\t\t\tif self.monitor_op(current, self.best):\n",
        "\t\t\t\t\t\tif self.verbose > 0:\n",
        "\t\t\t\t\t\t\t\tprint('\\nEpoch %05d: %s improved from %0.5f to %0.5f,' ' saving model to %s' % (epoch + 1, self.monitor, self.best, current, filepath))\n",
        "\t\t\t\t\t\tself.best = current\n",
        "\t\t\t\t\t\tif self.save_weights_only:\n",
        "\t\t\t\t\t\t\t\tself.model.save_weights(filepath, overwrite=True)\n",
        "\t\t\t\t\t\telse:\n",
        "\t\t\t\t\t\t\tself.model.save(filepath, overwrite=True)\n",
        "\t\t\t\t\t\t# Save model.h5 on to google storage\n",
        "\t\t\t\t\t\twith file_io.FileIO(filepath, mode='r') as input_f:\n",
        "\t\t\t\t\t\t\twith file_io.FileIO(self.folder + '/checkpoints/' + filepath, mode='w+') as output_f:\t# w+ : writing and reading\n",
        "\t\t\t\t\t\t\t\toutput_f.write(input_f.read())\n",
        "\t\t\t\telse:\n",
        "\t\t\t\t\tif self.verbose > 0:\n",
        "\t\t\t\t\t\t\tprint('\\nEpoch %05d: %s did not improve' %\n",
        "\t\t\t\t\t\t\t\t\t\t(epoch + 1, self.monitor))\n",
        "\t\telse:\n",
        "\t\t\tif self.verbose > 0:\n",
        "\t\t\t\t\tprint('\\nEpoch %05d: saving model to %s' % (epoch + 1, filepath))\n",
        "\t\t\tif self.save_weights_only:\n",
        "\t\t\t\t\tself.model.save_weights(filepath, overwrite=True)\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.model.save(filepath, overwrite=True)\n",
        "\t\t\t\t# Save model.h5 on to google storage\n",
        "\t\t\t\twith file_io.FileIO(filepath, mode='r') as input_f:\n",
        "\t\t\t\t\twith file_io.FileIO(self.folder + '/checkpoints/' + filepath, mode='w+') as output_f:\t# w+ : writing and reading\n",
        "\t\t\t\t\t\toutput_f.write(input_f.read())\n",
        "\n",
        "# Save the model after every epoch\n",
        "# filepath:       String, path to save the model file\n",
        "# monitor:        Quantity to monitor {val_loss, val_acc}\n",
        "# save_best_only: If save_best_only=True, the latest best model according to the quantity monitored will not be overwritten\n",
        "# mode:           One of {auto, min, max}. If save_best_only = True, the decision to overwrite the current save file is made based on either\n",
        "#\t\t\t      the maximization or the minimization of the monitored quantity. For val_acc, this should be max, for val_loss this should\n",
        "#\t\t\t      be min, etc. In auto mode, the direction is automatically inferred from the name of the monitored quantity\n",
        "# period:         Interval (number of epochs) between checkpoints\n",
        "check_point = ModelCheckpoint(\n",
        "filepath\t\t= 'Inception-v3_{epoch:02d}_{val_loss:.2f}.h5',\n",
        "folder \t\t\t= folder,\n",
        "monitor \t\t= 'val_loss', # Accuracy is not always a good indicator because of its yes or no nature\n",
        "save_best_only\t= True,\n",
        "mode \t\t\t= 'auto',\n",
        "period\t\t\t= 1)\n",
        "\n",
        "# We train our model again (this time fine-tuning all the inception blocks)\n",
        "model.fit_generator(\n",
        "\tgenerator           = train_generator,\n",
        "\tsteps_per_epoch     = len(train_data_x) // batch_size,  # samples_per_epoch / batch_size \n",
        "\tepochs              = epochs_all_layers,                        \n",
        "\tvalidation_data     = val_data,\n",
        "\tcallbacks           = [tensorboard_all_layers, reduce_lr, early_stop, check_point])\n",
        "\n",
        "# SAVING ##############################################################################################################################################\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "224/224 [==============================] - 245s 1s/step - loss: 1.5070 - acc: 0.4150 - val_loss: 1.4982 - val_acc: 0.4213\n",
            "Epoch 2/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.4431 - acc: 0.4463 - val_loss: 1.4451 - val_acc: 0.4422\n",
            "Epoch 3/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.3968 - acc: 0.4664 - val_loss: 1.4136 - val_acc: 0.4556\n",
            "Epoch 4/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.3610 - acc: 0.4811 - val_loss: 1.3791 - val_acc: 0.4773\n",
            "Epoch 5/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.3260 - acc: 0.4968 - val_loss: 1.3587 - val_acc: 0.4873\n",
            "Epoch 6/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.3034 - acc: 0.5026 - val_loss: 1.3289 - val_acc: 0.4960\n",
            "Epoch 7/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.2803 - acc: 0.5098 - val_loss: 1.3183 - val_acc: 0.4921\n",
            "Epoch 8/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.2580 - acc: 0.5218 - val_loss: 1.2942 - val_acc: 0.5074\n",
            "Epoch 9/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.2335 - acc: 0.5296 - val_loss: 1.2863 - val_acc: 0.5102\n",
            "Epoch 10/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.2169 - acc: 0.5370 - val_loss: 1.2665 - val_acc: 0.5163\n",
            "Epoch 11/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.1996 - acc: 0.5428 - val_loss: 1.2518 - val_acc: 0.5244\n",
            "Epoch 12/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.1816 - acc: 0.5503 - val_loss: 1.2453 - val_acc: 0.5261\n",
            "Epoch 13/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.1701 - acc: 0.5537 - val_loss: 1.2342 - val_acc: 0.5322\n",
            "Epoch 14/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.1480 - acc: 0.5654 - val_loss: 1.2198 - val_acc: 0.5355\n",
            "Epoch 15/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.1335 - acc: 0.5686 - val_loss: 1.2189 - val_acc: 0.5344\n",
            "Epoch 16/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.1211 - acc: 0.5742 - val_loss: 1.2033 - val_acc: 0.5447\n",
            "Epoch 17/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.1156 - acc: 0.5763 - val_loss: 1.1951 - val_acc: 0.5475\n",
            "Epoch 18/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.0931 - acc: 0.5868 - val_loss: 1.1932 - val_acc: 0.5489\n",
            "Epoch 19/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.0806 - acc: 0.5899 - val_loss: 1.1798 - val_acc: 0.5528\n",
            "Epoch 20/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.0703 - acc: 0.5947 - val_loss: 1.1751 - val_acc: 0.5545\n",
            "Epoch 21/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.0627 - acc: 0.5952 - val_loss: 1.1728 - val_acc: 0.5553\n",
            "Epoch 22/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.0429 - acc: 0.6041 - val_loss: 1.1626 - val_acc: 0.5626\n",
            "Epoch 23/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.0361 - acc: 0.6114 - val_loss: 1.1595 - val_acc: 0.5595\n",
            "Epoch 24/100\n",
            "224/224 [==============================] - 224s 1s/step - loss: 1.0249 - acc: 0.6126 - val_loss: 1.1536 - val_acc: 0.5639\n",
            "Epoch 25/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.0190 - acc: 0.6172 - val_loss: 1.1428 - val_acc: 0.5720\n",
            "Epoch 26/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.0089 - acc: 0.6207 - val_loss: 1.1378 - val_acc: 0.5770\n",
            "Epoch 27/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 1.0016 - acc: 0.6209 - val_loss: 1.1380 - val_acc: 0.5740\n",
            "Epoch 28/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.9789 - acc: 0.6299 - val_loss: 1.1331 - val_acc: 0.5743\n",
            "Epoch 29/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.9727 - acc: 0.6356 - val_loss: 1.1289 - val_acc: 0.5765\n",
            "Epoch 30/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.9646 - acc: 0.6365 - val_loss: 1.1197 - val_acc: 0.5826\n",
            "Epoch 31/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.9583 - acc: 0.6406 - val_loss: 1.1223 - val_acc: 0.5804\n",
            "Epoch 32/100\n",
            "224/224 [==============================] - 224s 1s/step - loss: 0.9418 - acc: 0.6465 - val_loss: 1.1126 - val_acc: 0.5885\n",
            "Epoch 33/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.9397 - acc: 0.6456 - val_loss: 1.1112 - val_acc: 0.5874\n",
            "Epoch 34/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.9257 - acc: 0.6543 - val_loss: 1.1093 - val_acc: 0.5921\n",
            "Epoch 35/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.9148 - acc: 0.6585 - val_loss: 1.1065 - val_acc: 0.5904\n",
            "Epoch 36/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.9075 - acc: 0.6650 - val_loss: 1.0977 - val_acc: 0.5960\n",
            "Epoch 37/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.9009 - acc: 0.6628 - val_loss: 1.1064 - val_acc: 0.5904\n",
            "Epoch 38/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.8906 - acc: 0.6660 - val_loss: 1.0985 - val_acc: 0.6004\n",
            "Epoch 39/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.8810 - acc: 0.6704 - val_loss: 1.0953 - val_acc: 0.6024\n",
            "Epoch 40/100\n",
            "224/224 [==============================] - 227s 1s/step - loss: 0.8710 - acc: 0.6764 - val_loss: 1.0904 - val_acc: 0.6043\n",
            "Epoch 41/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.8695 - acc: 0.6745 - val_loss: 1.0875 - val_acc: 0.6080\n",
            "Epoch 42/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.8509 - acc: 0.6818 - val_loss: 1.0861 - val_acc: 0.6046\n",
            "Epoch 43/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.8470 - acc: 0.6836 - val_loss: 1.0876 - val_acc: 0.6066\n",
            "Epoch 44/100\n",
            "224/224 [==============================] - 227s 1s/step - loss: 0.8358 - acc: 0.6874 - val_loss: 1.0872 - val_acc: 0.6088\n",
            "Epoch 45/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.8228 - acc: 0.6949 - val_loss: 1.0839 - val_acc: 0.6108\n",
            "Epoch 46/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.8189 - acc: 0.6992 - val_loss: 1.0820 - val_acc: 0.6127\n",
            "Epoch 47/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.8187 - acc: 0.6937 - val_loss: 1.0911 - val_acc: 0.6080\n",
            "Epoch 48/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.7969 - acc: 0.7062 - val_loss: 1.0832 - val_acc: 0.6099\n",
            "Epoch 49/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.7905 - acc: 0.7068 - val_loss: 1.0841 - val_acc: 0.6116\n",
            "Epoch 50/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.7830 - acc: 0.7115 - val_loss: 1.0817 - val_acc: 0.6127\n",
            "Epoch 51/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.7712 - acc: 0.7150 - val_loss: 1.0800 - val_acc: 0.6183\n",
            "Epoch 52/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.7624 - acc: 0.7170 - val_loss: 1.0787 - val_acc: 0.6186\n",
            "Epoch 53/100\n",
            "224/224 [==============================] - 227s 1s/step - loss: 0.7585 - acc: 0.7188 - val_loss: 1.0812 - val_acc: 0.6169\n",
            "Epoch 54/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.7485 - acc: 0.7234 - val_loss: 1.0854 - val_acc: 0.6191\n",
            "Epoch 55/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.7445 - acc: 0.7259 - val_loss: 1.0834 - val_acc: 0.6202\n",
            "Epoch 56/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.7279 - acc: 0.7288 - val_loss: 1.0802 - val_acc: 0.6255\n",
            "Epoch 57/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.7236 - acc: 0.7325 - val_loss: 1.0838 - val_acc: 0.6216\n",
            "Epoch 58/100\n",
            "224/224 [==============================] - 226s 1s/step - loss: 0.7145 - acc: 0.7360 - val_loss: 1.0798 - val_acc: 0.6230\n",
            "Epoch 59/100\n",
            "224/224 [==============================] - 224s 999ms/step - loss: 0.7050 - acc: 0.7420 - val_loss: 1.0823 - val_acc: 0.6258\n",
            "Epoch 60/100\n",
            "224/224 [==============================] - 223s 995ms/step - loss: 0.7070 - acc: 0.7377 - val_loss: 1.0838 - val_acc: 0.6236\n",
            "Epoch 61/100\n",
            "224/224 [==============================] - 224s 1s/step - loss: 0.6956 - acc: 0.7425 - val_loss: 1.0828 - val_acc: 0.6269\n",
            "Epoch 62/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.6970 - acc: 0.7443 - val_loss: 1.0825 - val_acc: 0.6255\n",
            "Epoch 63/100\n",
            "224/224 [==============================] - 224s 999ms/step - loss: 0.6985 - acc: 0.7435 - val_loss: 1.0834 - val_acc: 0.6266\n",
            "Epoch 64/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.6871 - acc: 0.7469 - val_loss: 1.0834 - val_acc: 0.6275\n",
            "Epoch 65/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.6886 - acc: 0.7447 - val_loss: 1.0834 - val_acc: 0.6278\n",
            "Epoch 66/100\n",
            "224/224 [==============================] - 224s 1s/step - loss: 0.6906 - acc: 0.7464 - val_loss: 1.0833 - val_acc: 0.6275\n",
            "Epoch 67/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.6875 - acc: 0.7451 - val_loss: 1.0833 - val_acc: 0.6300\n",
            "Epoch 68/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.6813 - acc: 0.7517 - val_loss: 1.0847 - val_acc: 0.6269\n",
            "Epoch 69/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.6853 - acc: 0.7508 - val_loss: 1.0837 - val_acc: 0.6261\n",
            "Epoch 70/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.6817 - acc: 0.7471 - val_loss: 1.0840 - val_acc: 0.6291\n",
            "Epoch 71/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.6813 - acc: 0.7521 - val_loss: 1.0846 - val_acc: 0.6280\n",
            "Epoch 72/100\n",
            "224/224 [==============================] - 225s 1s/step - loss: 0.6785 - acc: 0.7516 - val_loss: 1.0848 - val_acc: 0.6275\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "UnicodeDecodeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-ad3054ed2c85>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Inception-v3.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0minput_f\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mfile_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFileIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/Inception-v3.h5'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moutput_f\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# w+ : writing and reading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m                         \u001b[0moutput_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    126\u001b[0m       \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     return self._prepare_value(\n\u001b[0;32m--> 128\u001b[0;31m         pywrap_tensorflow.ReadFromStream(self._read_buf, length))\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m   @deprecation.deprecated_args(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/lib/io/file_io.py\u001b[0m in \u001b[0;36m_prepare_value\u001b[0;34m(self, val)\u001b[0m\n\u001b[1;32m     96\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_str_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/compat.py\u001b[0m in \u001b[0;36mas_str_any\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    121\u001b[0m   \"\"\"\n\u001b[1;32m    122\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mas_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/util/compat.py\u001b[0m in \u001b[0;36mas_text\u001b[0;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected binary or unicode string, got %r'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLsxQFLD86ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Saving the model in the workspace\n",
        "model.save('Inception-v3-us.h5')\n",
        "# Save model.h5 on to google storage\n",
        "with file_io.FileIO('Inception-v3.h5', mode='rb') as input_f:\n",
        "\twith file_io.FileIO(folder + '/Inception-v3.h5', mode='wb+') as output_f:  # w+ : writing and reading\n",
        "\t\t\toutput_f.write(input_f.read())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCRcDwNaxwnQ",
        "colab_type": "code",
        "outputId": "804a67c6-5ed7-4073-9772-88e14f646f94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.engine.training.Model at 0x7f1e685ef400>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVcd8mMbAck4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}